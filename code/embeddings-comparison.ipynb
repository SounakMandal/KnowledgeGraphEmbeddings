{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55a1888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:46:53.212371Z",
     "iopub.status.busy": "2023-04-17T05:46:53.211789Z",
     "iopub.status.idle": "2023-04-17T05:47:06.015254Z",
     "shell.execute_reply": "2023-04-17T05:47:06.013887Z"
    },
    "papermill": {
     "duration": 12.815065,
     "end_time": "2023-04-17T05:47:06.018478",
     "exception": false,
     "start_time": "2023-04-17T05:46:53.203413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\r\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from rdflib) (3.0.9)\r\n",
      "Requirement already satisfied: importlib-metadata<5.0.0,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from rdflib) (4.11.4)\r\n",
      "Collecting isodate<0.7.0,>=0.6.0\r\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0,>=4.0.0->rdflib) (4.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0,>=4.0.0->rdflib) (3.11.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\r\n",
      "Installing collected packages: isodate, rdflib\r\n",
      "Successfully installed isodate-0.6.1 rdflib-6.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8631116f",
   "metadata": {
    "papermill": {
     "duration": 0.005743,
     "end_time": "2023-04-17T05:47:06.030526",
     "exception": false,
     "start_time": "2023-04-17T05:47:06.024783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate Dataset\n",
    "This class creates the dataset from a list of file links. A variation of this class is used in all the notebooks in order to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d26594b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:06.044961Z",
     "iopub.status.busy": "2023-04-17T05:47:06.044049Z",
     "iopub.status.idle": "2023-04-17T05:47:06.544139Z",
     "shell.execute_reply": "2023-04-17T05:47:06.542923Z"
    },
    "papermill": {
     "duration": 0.510496,
     "end_time": "2023-04-17T05:47:06.546997",
     "exception": false,
     "start_time": "2023-04-17T05:47:06.036501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class TriplesDataset:\n",
    "    def __init__(self, url_list: str) -> None:\n",
    "        datapoints = []\n",
    "        labels = []\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        \n",
    "        for url in url_list:\n",
    "            graph_variable = rdflib.Graph()\n",
    "            resultGraph = graph_variable.parse(url)\n",
    "            for subject_item, predicate, object_item in resultGraph.triples((None, None, None)):\n",
    "                if type(object_item) != rdflib.term.URIRef:\n",
    "                    continue\n",
    "                \n",
    "                # add them to entities and relations\n",
    "                entities.add(str(subject_item))\n",
    "                entities.add(str(object_item))\n",
    "                relations.add(str(predicate))\n",
    "                \n",
    "                # add them to datapoints\n",
    "                datapoints.append(\n",
    "                    tuple([str(subject_item), str(object_item)])\n",
    "                )\n",
    "                labels.append(predicate)\n",
    "        \n",
    "        self.entities = list(entities)\n",
    "        self.relations = list(relations)\n",
    "        self.datapoints = datapoints\n",
    "        self.labels = labels\n",
    "        \n",
    "        print(f\"Entites : {len(self.entities)}\")\n",
    "        print(f\"Datapoints shape : {len(self.datapoints)}\")\n",
    "        print(f\"Labels = {len(self.labels)}\")\n",
    "    \n",
    "    def generate_negative_samples(self, count):\n",
    "        links_set = set(self.datapoints)\n",
    "        entities_set = self.entities\n",
    "        entities_count = len(entities_set)\n",
    "        \n",
    "        negative_samples = set()\n",
    "        while len(negative_samples) != count:\n",
    "            head_index, tail_index = random.sample(range(entities_count), 2)\n",
    "            head = entities_set[head_index]\n",
    "            tail = entities_set[tail_index]\n",
    "            possible_sample = tuple([head, tail])\n",
    "            if possible_sample not in links_set:\n",
    "                negative_samples.add(possible_sample)\n",
    "        return list(negative_samples)\n",
    "    \n",
    "    def construct_hash(self):\n",
    "        entity_hash = dict()\n",
    "        for index, entity in enumerate(self.entities):\n",
    "            entity_hash[entity] = index\n",
    "        \n",
    "        relation_hash = dict()\n",
    "        for index, relation in enumerate(self.relations):\n",
    "            relation_hash[relation] = index\n",
    "        return entity_hash, relation_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35d850",
   "metadata": {
    "papermill": {
     "duration": 0.005671,
     "end_time": "2023-04-17T05:47:06.559005",
     "exception": false,
     "start_time": "2023-04-17T05:47:06.553334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create model\n",
    "Create the base model that will be used to evaluate all the embeddings. The exact model does not matter since we expect better embeddings (embeddings that capture the semantics of the underlying data better) to perform better if we fix the model. Here we use a classifier model to perform link prediction, that is, given the embeddings of two nodes, the model tries to predict whether there exists some link between them. We use the simplest possible model that can make such a prediction to reduce the baseline to as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b69026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:06.573792Z",
     "iopub.status.busy": "2023-04-17T05:47:06.572884Z",
     "iopub.status.idle": "2023-04-17T05:47:09.126961Z",
     "shell.execute_reply": "2023-04-17T05:47:09.125672Z"
    },
    "papermill": {
     "duration": 2.565057,
     "end_time": "2023-04-17T05:47:09.130178",
     "exception": false,
     "start_time": "2023-04-17T05:47:06.565121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dimension, outputs):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(dimension, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512564f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.144956Z",
     "iopub.status.busy": "2023-04-17T05:47:09.144268Z",
     "iopub.status.idle": "2023-04-17T05:47:09.159657Z",
     "shell.execute_reply": "2023-04-17T05:47:09.158048Z"
    },
    "papermill": {
     "duration": 0.025486,
     "end_time": "2023-04-17T05:47:09.161965",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.136479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, hashing, margin):\n",
    "        super(TransE, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.entity_hash = hashing[0]\n",
    "        self.relation_hash = hashing[1]\n",
    "        self.margin = margin\n",
    "        \n",
    "        # define entity and relation embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)\n",
    "    \n",
    "    def _calculate_loss(self, triples):\n",
    "        score = []\n",
    "        for triple in triples:\n",
    "            head, relation, tail = triple\n",
    "            head_embedding = self.entity_embeddings(torch.tensor(self.entity_hash[head]))\n",
    "            relation_embedding = self.relation_embeddings(torch.tensor(self.relation_hash[relation]))\n",
    "            tail_embedding = self.entity_embeddings(torch.tensor(self.entity_hash[tail]))\n",
    "            distance = torch.linalg.norm(head_embedding + relation_embedding - tail_embedding)\n",
    "            score.append(distance)\n",
    "        return torch.tensor(score)\n",
    "    \n",
    "    def forward(self, positive_triples, negative_triples):\n",
    "        heads, predicates, tails = positive_triples\n",
    "        positive_score = self._calculate_loss(zip(heads, predicates, tails))\n",
    "        negative_score = self._calculate_loss(negative_triples)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = torch.mean(torch.max(\n",
    "            torch.zeros_like(negative_score), \n",
    "            self.margin + positive_score - negative_score\n",
    "        ))\n",
    "        loss.requires_grad = True\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abea93",
   "metadata": {
    "papermill": {
     "duration": 0.00563,
     "end_time": "2023-04-17T05:47:09.173664",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.168034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d98052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.187510Z",
     "iopub.status.busy": "2023-04-17T05:47:09.186737Z",
     "iopub.status.idle": "2023-04-17T05:47:09.195509Z",
     "shell.execute_reply": "2023-04-17T05:47:09.194100Z"
    },
    "papermill": {
     "duration": 0.018757,
     "end_time": "2023-04-17T05:47:09.198190",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.179433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Loop over the training data\n",
    "    for data, target in train_loader:\n",
    "        # Move the data to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42efc4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.212370Z",
     "iopub.status.busy": "2023-04-17T05:47:09.211925Z",
     "iopub.status.idle": "2023-04-17T05:47:09.218911Z",
     "shell.execute_reply": "2023-04-17T05:47:09.217754Z"
    },
    "papermill": {
     "duration": 0.017064,
     "end_time": "2023-04-17T05:47:09.221375",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.204311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    # Evaluate the model on some test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0.0\n",
    "        for test_input, label in val_loader:\n",
    "            output = model(test_input)  # use the model to make predictions on the test data\n",
    "            predictions = torch.tensor(output >= 0.5, dtype=float)\n",
    "            val_accuracy += torch.sum(predictions == label)\n",
    "        val_accuracy /= len(val_loader.dataset)\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05439b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.235940Z",
     "iopub.status.busy": "2023-04-17T05:47:09.235492Z",
     "iopub.status.idle": "2023-04-17T05:47:09.243267Z",
     "shell.execute_reply": "2023-04-17T05:47:09.242090Z"
    },
    "papermill": {
     "duration": 0.018599,
     "end_time": "2023-04-17T05:47:09.246086",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.227487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed(embeddings, datapoint, indicator):\n",
    "    head, tail = datapoint\n",
    "    if indicator == \"graph_walk\":\n",
    "        head_embedding = embeddings[head]\n",
    "        tail_embedding = embeddings[tail]\n",
    "    else:\n",
    "        head, tail = embeddings.entity_hash[head], embeddings.entity_hash[tail]\n",
    "        entity_embeddings = embeddings.entity_embeddings\n",
    "        head_embedding = entity_embeddings(torch.tensor(head)).detach().numpy()\n",
    "        tail_embedding = entity_embeddings(torch.tensor(tail)).detach().numpy()\n",
    "    return np.concatenate([head_embedding, tail_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f26b300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.262058Z",
     "iopub.status.busy": "2023-04-17T05:47:09.261261Z",
     "iopub.status.idle": "2023-04-17T05:47:09.272825Z",
     "shell.execute_reply": "2023-04-17T05:47:09.271539Z"
    },
    "papermill": {
     "duration": 0.023051,
     "end_time": "2023-04-17T05:47:09.275401",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.252350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7878d1ee12d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085ed45a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.290277Z",
     "iopub.status.busy": "2023-04-17T05:47:09.289481Z",
     "iopub.status.idle": "2023-04-17T05:47:09.302661Z",
     "shell.execute_reply": "2023-04-17T05:47:09.301325Z"
    },
    "papermill": {
     "duration": 0.023855,
     "end_time": "2023-04-17T05:47:09.305685",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.281830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "def generate_loaders(dataset, embeddings, indicator):\n",
    "    # Define the training data\n",
    "    positive_samples = dataset.datapoints\n",
    "    negative_samples = dataset.generate_negative_samples(len(positive_samples))\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    print(\"All samples size : \", len(all_samples))\n",
    "\n",
    "    x = map(lambda sample : embed(embeddings, sample, indicator), all_samples)\n",
    "    x_train = torch.tensor(np.array(list(x)))\n",
    "    print(\"Training dataset size\", x_train.shape)\n",
    "    \n",
    "    torch_labels = torch.tensor([1 for label in dataset.labels])\n",
    "    positive_labels = torch.ones_like(torch_labels)\n",
    "    negative_labels = torch.zeros_like(torch_labels)\n",
    "    all_labels = torch.cat([positive_labels, negative_labels])\n",
    "    y_train = torch.tensor(all_labels).float().unsqueeze(1)\n",
    "    print(\"Training labels size : \", y_train.shape)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "    # Define the sizes of the train and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # make dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c5fc97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.320502Z",
     "iopub.status.busy": "2023-04-17T05:47:09.320071Z",
     "iopub.status.idle": "2023-04-17T05:47:09.329815Z",
     "shell.execute_reply": "2023-04-17T05:47:09.328577Z"
    },
    "papermill": {
     "duration": 0.020257,
     "end_time": "2023-04-17T05:47:09.332352",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.312095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(dataset, embeddings, embeddings_length, indicator):\n",
    "    # Define the input and output dimensions\n",
    "    input_dimension = 2 * embeddings_length\n",
    "    output_dimension = 1\n",
    "\n",
    "    # Define the model, criterion, optimizer, and device\n",
    "    model = Classifier(input_dimension, output_dimension)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    \n",
    "    # generate data loaders\n",
    "    train_loader, val_loader = generate_loaders(dataset, embeddings, indicator)\n",
    "    \n",
    "    # Train the model\n",
    "    patience = 5\n",
    "    best_valid_accuracy = 0\n",
    "    epochs_without_improvement = 0\n",
    "    for epoch in range(100):\n",
    "        train(model, criterion, optimizer, train_loader, device)\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1} completed Validation accuracy : {int(val_accuracy * 1e4) / 1e2}\")\n",
    "\n",
    "        # Save the model\n",
    "        if val_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = val_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), f\"model_state_{indicator}.pth\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement == patience:\n",
    "                print(f'Early stopping after {epoch} epochs')\n",
    "                break\n",
    "    print(f\"Best Validation Accuracy : {best_valid_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425cb72",
   "metadata": {
    "papermill": {
     "duration": 0.006264,
     "end_time": "2023-04-17T05:47:09.345241",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.338977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34040416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:47:09.359923Z",
     "iopub.status.busy": "2023-04-17T05:47:09.359521Z",
     "iopub.status.idle": "2023-04-17T05:48:16.856550Z",
     "shell.execute_reply": "2023-04-17T05:48:16.855337Z"
    },
    "papermill": {
     "duration": 67.513955,
     "end_time": "2023-04-17T05:48:16.865544",
     "exception": false,
     "start_time": "2023-04-17T05:47:09.351589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entites : 44740\n",
      "Datapoints shape : 297285\n",
      "Labels = 297285\n"
     ]
    }
   ],
   "source": [
    "dataFilePath = [\n",
    "    \"/kaggle/input/bgs-dataset/625KGeologyMap_all.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/dataholdings.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/earth-material-class.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/geochronology.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/lexicon-named-rock-unit.nt\"\n",
    "]\n",
    "dataset = TriplesDataset(dataFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ed3dc",
   "metadata": {
    "papermill": {
     "duration": 0.006251,
     "end_time": "2023-04-17T05:48:16.878425",
     "exception": false,
     "start_time": "2023-04-17T05:48:16.872174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate random embeddings\n",
    "The TransE model starts with random embeddings for each entity. By using the raw embeddings and not using the trained embeddings, we can effective test how well the classifier model works on random embeddings and then set a baseline for other trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ab5a3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:48:16.893630Z",
     "iopub.status.busy": "2023-04-17T05:48:16.893201Z",
     "iopub.status.idle": "2023-04-17T05:48:17.050466Z",
     "shell.execute_reply": "2023-04-17T05:48:17.049064Z"
    },
    "papermill": {
     "duration": 0.168733,
     "end_time": "2023-04-17T05:48:17.053644",
     "exception": false,
     "start_time": "2023-04-17T05:48:16.884911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_entities = len(dataset.entities)\n",
    "num_relations = len(dataset.relations)\n",
    "embedding_dim = 100\n",
    "hashing = dataset.construct_hash()\n",
    "margin = 0.5\n",
    "randomEmbeddings = TransE(num_entities, num_relations, embedding_dim, hashing, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4421624c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:48:17.070150Z",
     "iopub.status.busy": "2023-04-17T05:48:17.069034Z",
     "iopub.status.idle": "2023-04-17T05:52:21.935827Z",
     "shell.execute_reply": "2023-04-17T05:52:21.934450Z"
    },
    "papermill": {
     "duration": 244.885956,
     "end_time": "2023-04-17T05:52:21.946577",
     "exception": false,
     "start_time": "2023-04-17T05:48:17.060621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples size :  594570\n",
      "Training dataset size torch.Size([594570, 200])\n",
      "Training labels size :  torch.Size([594570, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed Validation accuracy : 65.59\n",
      "Epoch 2 completed Validation accuracy : 65.98\n",
      "Epoch 3 completed Validation accuracy : 66.37\n",
      "Epoch 4 completed Validation accuracy : 67.24\n",
      "Epoch 5 completed Validation accuracy : 68.15\n",
      "Epoch 6 completed Validation accuracy : 68.87\n",
      "Epoch 7 completed Validation accuracy : 69.35\n",
      "Epoch 8 completed Validation accuracy : 69.72\n",
      "Epoch 9 completed Validation accuracy : 69.87\n",
      "Epoch 10 completed Validation accuracy : 70.03\n",
      "Epoch 11 completed Validation accuracy : 70.18\n",
      "Epoch 12 completed Validation accuracy : 70.33\n",
      "Epoch 13 completed Validation accuracy : 70.43\n",
      "Epoch 14 completed Validation accuracy : 70.6\n",
      "Epoch 15 completed Validation accuracy : 70.6\n",
      "Epoch 16 completed Validation accuracy : 70.54\n",
      "Epoch 17 completed Validation accuracy : 70.59\n",
      "Epoch 18 completed Validation accuracy : 70.65\n",
      "Epoch 19 completed Validation accuracy : 70.68\n",
      "Epoch 20 completed Validation accuracy : 70.72\n",
      "Epoch 21 completed Validation accuracy : 70.79\n",
      "Epoch 22 completed Validation accuracy : 70.85\n",
      "Epoch 23 completed Validation accuracy : 70.87\n",
      "Epoch 24 completed Validation accuracy : 70.87\n",
      "Epoch 25 completed Validation accuracy : 71.09\n",
      "Epoch 26 completed Validation accuracy : 70.89\n",
      "Epoch 27 completed Validation accuracy : 70.89\n",
      "Epoch 28 completed Validation accuracy : 70.9\n",
      "Epoch 29 completed Validation accuracy : 70.9\n",
      "Epoch 30 completed Validation accuracy : 71.05\n",
      "Early stopping after 29 epochs\n",
      "Best Validation Accuracy : 0.7109844088554382\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    dataset, \n",
    "    randomEmbeddings, \n",
    "    embedding_dim,\n",
    "    \"random\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335492f",
   "metadata": {
    "papermill": {
     "duration": 0.008437,
     "end_time": "2023-04-17T05:52:21.963961",
     "exception": false,
     "start_time": "2023-04-17T05:52:21.955524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the TransE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e026b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:52:21.983846Z",
     "iopub.status.busy": "2023-04-17T05:52:21.983402Z",
     "iopub.status.idle": "2023-04-17T05:52:22.444671Z",
     "shell.execute_reply": "2023-04-17T05:52:22.443650Z"
    },
    "papermill": {
     "duration": 0.474355,
     "end_time": "2023-04-17T05:52:22.447213",
     "exception": false,
     "start_time": "2023-04-17T05:52:21.972858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_entities = len(dataset.entities)\n",
    "num_relations = len(dataset.relations)\n",
    "embedding_dim = 100\n",
    "hashing = dataset.construct_hash()\n",
    "margin = 0.5\n",
    "transeEmbeddings = TransE(num_entities, num_relations, embedding_dim, hashing, margin)\n",
    "state_dict = torch.load(\"/kaggle/input/knowledgegraphembeddings/transeEmbeddings.pth\")\n",
    "transeEmbeddings.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65c37cf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:52:22.468960Z",
     "iopub.status.busy": "2023-04-17T05:52:22.468173Z",
     "iopub.status.idle": "2023-04-17T05:58:38.041334Z",
     "shell.execute_reply": "2023-04-17T05:58:38.040009Z"
    },
    "papermill": {
     "duration": 375.586151,
     "end_time": "2023-04-17T05:58:38.043891",
     "exception": false,
     "start_time": "2023-04-17T05:52:22.457740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples size :  594570\n",
      "Training dataset size torch.Size([594570, 200])\n",
      "Training labels size :  torch.Size([594570, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed Validation accuracy : 67.01\n",
      "Epoch 2 completed Validation accuracy : 68.23\n",
      "Epoch 3 completed Validation accuracy : 69.03\n",
      "Epoch 4 completed Validation accuracy : 69.29\n",
      "Epoch 5 completed Validation accuracy : 69.6\n",
      "Epoch 6 completed Validation accuracy : 69.91\n",
      "Epoch 7 completed Validation accuracy : 70.17\n",
      "Epoch 8 completed Validation accuracy : 70.3\n",
      "Epoch 9 completed Validation accuracy : 70.55\n",
      "Epoch 10 completed Validation accuracy : 70.76\n",
      "Epoch 11 completed Validation accuracy : 70.97\n",
      "Epoch 12 completed Validation accuracy : 71.03\n",
      "Epoch 13 completed Validation accuracy : 71.27\n",
      "Epoch 14 completed Validation accuracy : 71.39\n",
      "Epoch 15 completed Validation accuracy : 71.5\n",
      "Epoch 16 completed Validation accuracy : 71.56\n",
      "Epoch 17 completed Validation accuracy : 71.55\n",
      "Epoch 18 completed Validation accuracy : 71.62\n",
      "Epoch 19 completed Validation accuracy : 71.72\n",
      "Epoch 20 completed Validation accuracy : 71.75\n",
      "Epoch 21 completed Validation accuracy : 71.82\n",
      "Epoch 22 completed Validation accuracy : 71.8\n",
      "Epoch 23 completed Validation accuracy : 71.91\n",
      "Epoch 24 completed Validation accuracy : 71.91\n",
      "Epoch 25 completed Validation accuracy : 71.79\n",
      "Epoch 26 completed Validation accuracy : 71.93\n",
      "Epoch 27 completed Validation accuracy : 71.93\n",
      "Epoch 28 completed Validation accuracy : 71.96\n",
      "Epoch 29 completed Validation accuracy : 71.92\n",
      "Epoch 30 completed Validation accuracy : 71.96\n",
      "Epoch 31 completed Validation accuracy : 72.02\n",
      "Epoch 32 completed Validation accuracy : 71.98\n",
      "Epoch 33 completed Validation accuracy : 72.01\n",
      "Epoch 34 completed Validation accuracy : 72.02\n",
      "Epoch 35 completed Validation accuracy : 72.03\n",
      "Epoch 36 completed Validation accuracy : 72.01\n",
      "Epoch 37 completed Validation accuracy : 72.01\n",
      "Epoch 38 completed Validation accuracy : 72.05\n",
      "Epoch 39 completed Validation accuracy : 72.05\n",
      "Epoch 40 completed Validation accuracy : 72.03\n",
      "Epoch 41 completed Validation accuracy : 72.03\n",
      "Epoch 42 completed Validation accuracy : 72.1\n",
      "Epoch 43 completed Validation accuracy : 72.11\n",
      "Epoch 44 completed Validation accuracy : 72.11\n",
      "Epoch 45 completed Validation accuracy : 72.09\n",
      "Epoch 46 completed Validation accuracy : 72.01\n",
      "Epoch 47 completed Validation accuracy : 72.05\n",
      "Epoch 48 completed Validation accuracy : 72.05\n",
      "Epoch 49 completed Validation accuracy : 72.06\n",
      "Early stopping after 48 epochs\n",
      "Best Validation Accuracy : 0.7211430072784424\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    dataset, \n",
    "    transeEmbeddings, \n",
    "    embedding_dim,\n",
    "    \"transE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc1e13",
   "metadata": {
    "papermill": {
     "duration": 0.011594,
     "end_time": "2023-04-17T05:58:38.067544",
     "exception": false,
     "start_time": "2023-04-17T05:58:38.055950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the Rdf2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a347b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:58:38.094138Z",
     "iopub.status.busy": "2023-04-17T05:58:38.092888Z",
     "iopub.status.idle": "2023-04-17T05:58:39.926406Z",
     "shell.execute_reply": "2023-04-17T05:58:39.925331Z"
    },
    "papermill": {
     "duration": 1.849647,
     "end_time": "2023-04-17T05:58:39.929268",
     "exception": false,
     "start_time": "2023-04-17T05:58:38.079621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filePath = '/kaggle/input/knowledgegraphembeddings/nodeEmbeddings100.bin'\n",
    "embeddings = KeyedVectors.load_word2vec_format(filePath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "055415e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T05:58:39.956410Z",
     "iopub.status.busy": "2023-04-17T05:58:39.955288Z",
     "iopub.status.idle": "2023-04-17T06:00:54.104288Z",
     "shell.execute_reply": "2023-04-17T06:00:54.102826Z"
    },
    "papermill": {
     "duration": 134.166255,
     "end_time": "2023-04-17T06:00:54.107752",
     "exception": false,
     "start_time": "2023-04-17T05:58:39.941497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples size :  594570\n",
      "Training dataset size torch.Size([594570, 200])\n",
      "Training labels size :  torch.Size([594570, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed Validation accuracy : 92.44\n",
      "Epoch 2 completed Validation accuracy : 92.64\n",
      "Epoch 3 completed Validation accuracy : 92.73\n",
      "Epoch 4 completed Validation accuracy : 92.76\n",
      "Epoch 5 completed Validation accuracy : 92.8\n",
      "Epoch 6 completed Validation accuracy : 92.74\n",
      "Epoch 7 completed Validation accuracy : 92.84\n",
      "Epoch 8 completed Validation accuracy : 92.88\n",
      "Epoch 9 completed Validation accuracy : 92.86\n",
      "Epoch 10 completed Validation accuracy : 92.86\n",
      "Epoch 11 completed Validation accuracy : 92.85\n",
      "Epoch 12 completed Validation accuracy : 92.9\n",
      "Epoch 13 completed Validation accuracy : 92.89\n",
      "Epoch 14 completed Validation accuracy : 92.82\n",
      "Epoch 15 completed Validation accuracy : 92.9\n",
      "Epoch 16 completed Validation accuracy : 92.88\n",
      "Epoch 17 completed Validation accuracy : 92.89\n",
      "Early stopping after 16 epochs\n",
      "Best Validation Accuracy : 0.9290663599967957\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    dataset, \n",
    "    embeddings, \n",
    "    embeddings.vectors.shape[1], \n",
    "    \"graph_walk\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c72954",
   "metadata": {
    "papermill": {
     "duration": 0.014628,
     "end_time": "2023-04-17T06:00:54.137662",
     "exception": false,
     "start_time": "2023-04-17T06:00:54.123034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 854.185502,
   "end_time": "2023-04-17T06:00:56.776010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-17T05:46:42.590508",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
