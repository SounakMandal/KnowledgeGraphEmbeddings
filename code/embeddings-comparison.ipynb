{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fd99db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:33.925597Z",
     "iopub.status.busy": "2023-04-16T18:23:33.925194Z",
     "iopub.status.idle": "2023-04-16T18:23:46.620796Z",
     "shell.execute_reply": "2023-04-16T18:23:46.619036Z"
    },
    "papermill": {
     "duration": 12.70662,
     "end_time": "2023-04-16T18:23:46.624329",
     "exception": false,
     "start_time": "2023-04-16T18:23:33.917709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\r\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata<5.0.0,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from rdflib) (4.11.4)\r\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from rdflib) (3.0.9)\r\n",
      "Collecting isodate<0.7.0,>=0.6.0\r\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0,>=4.0.0->rdflib) (3.11.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0,>=4.0.0->rdflib) (4.4.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\r\n",
      "Installing collected packages: isodate, rdflib\r\n",
      "Successfully installed isodate-0.6.1 rdflib-6.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecd15cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:46.638109Z",
     "iopub.status.busy": "2023-04-16T18:23:46.636944Z",
     "iopub.status.idle": "2023-04-16T18:23:47.153925Z",
     "shell.execute_reply": "2023-04-16T18:23:47.152625Z"
    },
    "papermill": {
     "duration": 0.526947,
     "end_time": "2023-04-16T18:23:47.156894",
     "exception": false,
     "start_time": "2023-04-16T18:23:46.629947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class TriplesDataset:\n",
    "    def __init__(self, url_list: str) -> None:\n",
    "        datapoints = []\n",
    "        labels = []\n",
    "        entities = set()\n",
    "        relations = set()\n",
    "        \n",
    "        for url in url_list:\n",
    "            graph_variable = rdflib.Graph()\n",
    "            resultGraph = graph_variable.parse(url)\n",
    "            for subject_item, predicate, object_item in resultGraph.triples((None, None, None)):\n",
    "                if type(object_item) != rdflib.term.URIRef:\n",
    "                    continue\n",
    "                \n",
    "                # add them to entities and relations\n",
    "                entities.add(str(subject_item))\n",
    "                entities.add(str(object_item))\n",
    "                relations.add(str(predicate))\n",
    "                \n",
    "                # add them to datapoints\n",
    "                datapoints.append(\n",
    "                    tuple([str(subject_item), str(object_item)])\n",
    "                )\n",
    "                labels.append(predicate)\n",
    "        \n",
    "        self.entities = list(entities)\n",
    "        self.relations = list(relations)\n",
    "        self.datapoints = datapoints\n",
    "        self.labels = labels\n",
    "        \n",
    "        print(f\"Entites : {len(self.entities)}\")\n",
    "        print(f\"Datapoints shape : {len(self.datapoints)}\")\n",
    "        print(f\"Labels = {len(self.labels)}\")\n",
    "    \n",
    "    def generate_negative_samples(self, count):\n",
    "        links_set = set(self.datapoints)\n",
    "        entities_set = self.entities\n",
    "        entities_count = len(entities_set)\n",
    "        \n",
    "        negative_samples = set()\n",
    "        while len(negative_samples) != count:\n",
    "            head_index, tail_index = random.sample(range(entities_count), 2)\n",
    "            head = entities_set[head_index]\n",
    "            tail = entities_set[tail_index]\n",
    "            possible_sample = tuple([head, tail])\n",
    "            if possible_sample not in links_set:\n",
    "                negative_samples.add(possible_sample)\n",
    "        return list(negative_samples)\n",
    "    \n",
    "    def construct_hash(self):\n",
    "        entity_hash = dict()\n",
    "        for index, entity in enumerate(self.entities):\n",
    "            entity_hash[entity] = index\n",
    "        \n",
    "        relation_hash = dict()\n",
    "        for index, relation in enumerate(self.relations):\n",
    "            relation_hash[relation] = index\n",
    "        return entity_hash, relation_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a041c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:47.169175Z",
     "iopub.status.busy": "2023-04-16T18:23:47.168758Z",
     "iopub.status.idle": "2023-04-16T18:23:49.478046Z",
     "shell.execute_reply": "2023-04-16T18:23:49.477095Z"
    },
    "papermill": {
     "duration": 2.318553,
     "end_time": "2023-04-16T18:23:49.480738",
     "exception": false,
     "start_time": "2023-04-16T18:23:47.162185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dimension, outputs):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimension, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f23ed35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.494104Z",
     "iopub.status.busy": "2023-04-16T18:23:49.493170Z",
     "iopub.status.idle": "2023-04-16T18:23:49.506170Z",
     "shell.execute_reply": "2023-04-16T18:23:49.505319Z"
    },
    "papermill": {
     "duration": 0.022404,
     "end_time": "2023-04-16T18:23:49.508610",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.486206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, hashing, margin):\n",
    "        super(TransE, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.entity_hash = hashing[0]\n",
    "        self.relation_hash = hashing[1]\n",
    "        self.margin = margin\n",
    "        \n",
    "        # define entity and relation embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)\n",
    "    \n",
    "    def _calculate_loss(self, triples):\n",
    "        score = []\n",
    "        for triple in triples:\n",
    "            head, relation, tail = triple\n",
    "            head_embedding = self.entity_embeddings(torch.tensor(self.entity_hash[head]))\n",
    "            relation_embedding = self.relation_embeddings(torch.tensor(self.relation_hash[relation]))\n",
    "            tail_embedding = self.entity_embeddings(torch.tensor(self.entity_hash[tail]))\n",
    "            distance = torch.linalg.norm(head_embedding + relation_embedding - tail_embedding)\n",
    "            score.append(distance)\n",
    "        return torch.tensor(score)\n",
    "    \n",
    "    def forward(self, positive_triples, negative_triples):\n",
    "        heads, predicates, tails = positive_triples\n",
    "        positive_score = self._calculate_loss(zip(heads, predicates, tails))\n",
    "        negative_score = self._calculate_loss(negative_triples)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = torch.mean(torch.max(\n",
    "            torch.zeros_like(negative_score), \n",
    "            self.margin + positive_score - negative_score\n",
    "        ))\n",
    "        loss.requires_grad = True\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba82249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.521112Z",
     "iopub.status.busy": "2023-04-16T18:23:49.520143Z",
     "iopub.status.idle": "2023-04-16T18:23:49.527502Z",
     "shell.execute_reply": "2023-04-16T18:23:49.526350Z"
    },
    "papermill": {
     "duration": 0.016119,
     "end_time": "2023-04-16T18:23:49.530027",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.513908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Loop over the training data\n",
    "    for data, target in train_loader:\n",
    "        # Move the data to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9e5e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.542432Z",
     "iopub.status.busy": "2023-04-16T18:23:49.542036Z",
     "iopub.status.idle": "2023-04-16T18:23:49.548562Z",
     "shell.execute_reply": "2023-04-16T18:23:49.547319Z"
    },
    "papermill": {
     "duration": 0.015544,
     "end_time": "2023-04-16T18:23:49.550819",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.535275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    # Evaluate the model on some test data\n",
    "    with torch.no_grad():\n",
    "        val_accuracy = 0.0\n",
    "        for test_input, label in val_loader:\n",
    "            output = model(test_input)  # use the model to make predictions on the test data\n",
    "            predictions = torch.tensor(output >= 0.5, dtype=float)\n",
    "            val_accuracy += torch.sum(predictions == label)\n",
    "        val_accuracy /= len(val_loader.dataset)\n",
    "    print(\"Accuracy : \", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef1d7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.563286Z",
     "iopub.status.busy": "2023-04-16T18:23:49.562906Z",
     "iopub.status.idle": "2023-04-16T18:23:49.570073Z",
     "shell.execute_reply": "2023-04-16T18:23:49.569041Z"
    },
    "papermill": {
     "duration": 0.01664,
     "end_time": "2023-04-16T18:23:49.572668",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.556028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed(embeddings, datapoint, indicator):\n",
    "    head, tail = datapoint\n",
    "    if indicator == \"graph_walk\":\n",
    "        head_embedding = embeddings[head]\n",
    "        tail_embedding = embeddings[tail]\n",
    "    else:\n",
    "        head, tail = embeddings.entity_hash[head], embeddings.entity_hash[tail]\n",
    "        entity_embeddings = embeddings.entity_embeddings\n",
    "        head_embedding = entity_embeddings(torch.tensor(head)).detach().numpy()\n",
    "        tail_embedding = entity_embeddings(torch.tensor(tail)).detach().numpy()\n",
    "    return np.concatenate([head_embedding, tail_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4480b892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.584989Z",
     "iopub.status.busy": "2023-04-16T18:23:49.584606Z",
     "iopub.status.idle": "2023-04-16T18:23:49.593629Z",
     "shell.execute_reply": "2023-04-16T18:23:49.592655Z"
    },
    "papermill": {
     "duration": 0.017803,
     "end_time": "2023-04-16T18:23:49.595644",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.577841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72497eb892b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf24d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.607438Z",
     "iopub.status.busy": "2023-04-16T18:23:49.607068Z",
     "iopub.status.idle": "2023-04-16T18:23:49.618230Z",
     "shell.execute_reply": "2023-04-16T18:23:49.617189Z"
    },
    "papermill": {
     "duration": 0.019671,
     "end_time": "2023-04-16T18:23:49.620446",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.600775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "def generate_loaders(dataset, embeddings, indicator):\n",
    "    # Define the training data\n",
    "    positive_samples = dataset.datapoints\n",
    "    negative_samples = dataset.generate_negative_samples(len(positive_samples))\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    print(\"All samples size : \", len(all_samples))\n",
    "\n",
    "    x = map(lambda sample : embed(embeddings, sample, indicator), all_samples)\n",
    "    x_train = torch.tensor(np.array(list(x)))\n",
    "    print(\"Training dataset size\", x_train.shape)\n",
    "    \n",
    "    torch_labels = torch.tensor([1 for label in dataset.labels])\n",
    "    positive_labels = torch.ones_like(torch_labels)\n",
    "    negative_labels = torch.zeros_like(torch_labels)\n",
    "    all_labels = torch.cat([positive_labels, negative_labels])\n",
    "    y_train = torch.tensor(all_labels).float().unsqueeze(1)\n",
    "    print(\"Training labels size : \", y_train.shape)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "    # Define the sizes of the train and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # make dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe45490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.632816Z",
     "iopub.status.busy": "2023-04-16T18:23:49.632417Z",
     "iopub.status.idle": "2023-04-16T18:23:49.640939Z",
     "shell.execute_reply": "2023-04-16T18:23:49.639680Z"
    },
    "papermill": {
     "duration": 0.017463,
     "end_time": "2023-04-16T18:23:49.643393",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.625930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(dataset, embeddings, embeddings_length, indicator):\n",
    "    # Define the input and output dimensions\n",
    "    input_dimension = 2 * embeddings_length\n",
    "    output_dimension = 1\n",
    "\n",
    "    # Define the model, criterion, optimizer, and device\n",
    "    model = Classifier(input_dimension, output_dimension)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "    \n",
    "    # generate data loaders\n",
    "    train_loader, val_loader = generate_loaders(dataset, embeddings, indicator)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(50):\n",
    "        train(model, criterion, optimizer, train_loader, device)\n",
    "        print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"model_state_{indicator}.pth\")\n",
    "    \n",
    "    # evaluate the model\n",
    "    evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583cd72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:23:49.656167Z",
     "iopub.status.busy": "2023-04-16T18:23:49.655299Z",
     "iopub.status.idle": "2023-04-16T18:24:57.566531Z",
     "shell.execute_reply": "2023-04-16T18:24:57.563777Z"
    },
    "papermill": {
     "duration": 67.924605,
     "end_time": "2023-04-16T18:24:57.573381",
     "exception": false,
     "start_time": "2023-04-16T18:23:49.648776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entites : 44740\n",
      "Datapoints shape : 297285\n",
      "Labels = 297285\n"
     ]
    }
   ],
   "source": [
    "dataFilePath = [\n",
    "    \"/kaggle/input/bgs-dataset/625KGeologyMap_all.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/dataholdings.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/earth-material-class.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/geochronology.nt\",\n",
    "    \"/kaggle/input/bgs-dataset/lexicon-named-rock-unit.nt\"\n",
    "]\n",
    "dataset = TriplesDataset(dataFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b2f611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:24:57.586364Z",
     "iopub.status.busy": "2023-04-16T18:24:57.585575Z",
     "iopub.status.idle": "2023-04-16T18:24:59.400822Z",
     "shell.execute_reply": "2023-04-16T18:24:59.399644Z"
    },
    "papermill": {
     "duration": 1.824585,
     "end_time": "2023-04-16T18:24:59.403479",
     "exception": false,
     "start_time": "2023-04-16T18:24:57.578894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filePath = '/kaggle/input/knowledgegraphembeddings/nodeEmbeddings100.bin'\n",
    "embeddings = KeyedVectors.load_word2vec_format(filePath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7314ba19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:24:59.415837Z",
     "iopub.status.busy": "2023-04-16T18:24:59.415444Z",
     "iopub.status.idle": "2023-04-16T18:25:00.044313Z",
     "shell.execute_reply": "2023-04-16T18:25:00.043368Z"
    },
    "papermill": {
     "duration": 0.637812,
     "end_time": "2023-04-16T18:25:00.046751",
     "exception": false,
     "start_time": "2023-04-16T18:24:59.408939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_entities = len(dataset.entities)\n",
    "num_relations = len(dataset.relations)\n",
    "embedding_dim = 100\n",
    "hashing = dataset.construct_hash()\n",
    "margin = 0.5\n",
    "transeEmbeddings = TransE(num_entities, num_relations, embedding_dim, hashing, margin)\n",
    "state_dict = torch.load(\"/kaggle/input/knowledgegraphembeddings/transeEmbeddings.pth\")\n",
    "transeEmbeddings.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52a9bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:25:00.059991Z",
     "iopub.status.busy": "2023-04-16T18:25:00.058894Z",
     "iopub.status.idle": "2023-04-16T18:33:04.543335Z",
     "shell.execute_reply": "2023-04-16T18:33:04.542125Z"
    },
    "papermill": {
     "duration": 484.493416,
     "end_time": "2023-04-16T18:33:04.545733",
     "exception": false,
     "start_time": "2023-04-16T18:25:00.052317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples size :  594570\n",
      "Training dataset size torch.Size([594570, 200])\n",
      "Training labels size :  torch.Size([594570, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Epoch 4 completed\n",
      "Epoch 5 completed\n",
      "Epoch 6 completed\n",
      "Epoch 7 completed\n",
      "Epoch 8 completed\n",
      "Epoch 9 completed\n",
      "Epoch 10 completed\n",
      "Epoch 11 completed\n",
      "Epoch 12 completed\n",
      "Epoch 13 completed\n",
      "Epoch 14 completed\n",
      "Epoch 15 completed\n",
      "Epoch 16 completed\n",
      "Epoch 17 completed\n",
      "Epoch 18 completed\n",
      "Epoch 19 completed\n",
      "Epoch 20 completed\n",
      "Epoch 21 completed\n",
      "Epoch 22 completed\n",
      "Epoch 23 completed\n",
      "Epoch 24 completed\n",
      "Epoch 25 completed\n",
      "Epoch 26 completed\n",
      "Epoch 27 completed\n",
      "Epoch 28 completed\n",
      "Epoch 29 completed\n",
      "Epoch 30 completed\n",
      "Epoch 31 completed\n",
      "Epoch 32 completed\n",
      "Epoch 33 completed\n",
      "Epoch 34 completed\n",
      "Epoch 35 completed\n",
      "Epoch 36 completed\n",
      "Epoch 37 completed\n",
      "Epoch 38 completed\n",
      "Epoch 39 completed\n",
      "Epoch 40 completed\n",
      "Epoch 41 completed\n",
      "Epoch 42 completed\n",
      "Epoch 43 completed\n",
      "Epoch 44 completed\n",
      "Epoch 45 completed\n",
      "Epoch 46 completed\n",
      "Epoch 47 completed\n",
      "Epoch 48 completed\n",
      "Epoch 49 completed\n",
      "Epoch 50 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  tensor(0.9229)\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    dataset, \n",
    "    transeEmbeddings, \n",
    "    embedding_dim,\n",
    "    \"transE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5e4076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T18:33:04.565507Z",
     "iopub.status.busy": "2023-04-16T18:33:04.564847Z",
     "iopub.status.idle": "2023-04-16T18:40:38.845906Z",
     "shell.execute_reply": "2023-04-16T18:40:38.844381Z"
    },
    "papermill": {
     "duration": 454.293889,
     "end_time": "2023-04-16T18:40:38.848395",
     "exception": false,
     "start_time": "2023-04-16T18:33:04.554506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples size :  594570\n",
      "Training dataset size torch.Size([594570, 200])\n",
      "Training labels size :  torch.Size([594570, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Epoch 4 completed\n",
      "Epoch 5 completed\n",
      "Epoch 6 completed\n",
      "Epoch 7 completed\n",
      "Epoch 8 completed\n",
      "Epoch 9 completed\n",
      "Epoch 10 completed\n",
      "Epoch 11 completed\n",
      "Epoch 12 completed\n",
      "Epoch 13 completed\n",
      "Epoch 14 completed\n",
      "Epoch 15 completed\n",
      "Epoch 16 completed\n",
      "Epoch 17 completed\n",
      "Epoch 18 completed\n",
      "Epoch 19 completed\n",
      "Epoch 20 completed\n",
      "Epoch 21 completed\n",
      "Epoch 22 completed\n",
      "Epoch 23 completed\n",
      "Epoch 24 completed\n",
      "Epoch 25 completed\n",
      "Epoch 26 completed\n",
      "Epoch 27 completed\n",
      "Epoch 28 completed\n",
      "Epoch 29 completed\n",
      "Epoch 30 completed\n",
      "Epoch 31 completed\n",
      "Epoch 32 completed\n",
      "Epoch 33 completed\n",
      "Epoch 34 completed\n",
      "Epoch 35 completed\n",
      "Epoch 36 completed\n",
      "Epoch 37 completed\n",
      "Epoch 38 completed\n",
      "Epoch 39 completed\n",
      "Epoch 40 completed\n",
      "Epoch 41 completed\n",
      "Epoch 42 completed\n",
      "Epoch 43 completed\n",
      "Epoch 44 completed\n",
      "Epoch 45 completed\n",
      "Epoch 46 completed\n",
      "Epoch 47 completed\n",
      "Epoch 48 completed\n",
      "Epoch 49 completed\n",
      "Epoch 50 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  tensor(0.9860)\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    dataset, \n",
    "    embeddings, \n",
    "    embeddings.vectors.shape[1], \n",
    "    \"graph_walk\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5a727",
   "metadata": {
    "papermill": {
     "duration": 0.011626,
     "end_time": "2023-04-16T18:40:38.872000",
     "exception": false,
     "start_time": "2023-04-16T18:40:38.860374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1038.221363,
   "end_time": "2023-04-16T18:40:41.507744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-16T18:23:23.286381",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
